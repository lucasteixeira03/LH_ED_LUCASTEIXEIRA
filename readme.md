# **Indicium - Desafio de Engenharia de Dados**

## ğŸ™‹ğŸ»â€â™‚ï¸ ApresentaÃ§Ã£o Pessoal
#### OlÃ¡, me chamo Lucas De Sousa Teixeira, tenho 21 anos e sou da cidade de Tapejara, no estado do Rio Grande do Sul. Atualmente, estou cursando CiÃªncia da ComputaÃ§Ã£o na universidade do IFSUL - Campus Passo fundo.

---

## ğŸ“Œ Objetivo do Projeto
O objetivo deste projeto Ã© construir um pipeline de **extraÃ§Ã£o**, **transformaÃ§Ã£o** e **carga** (ETL) para centralizar dados do banco fictÃ­cio **BanVic** em um Data Warehouse (DW) e orquestrar sua execuÃ§Ã£o com o **Apache Airflow**.

Requisitos:

- Utilize o **Apache Airflow** (2 ou 3) como orquestrador de tarefas;

- As **extraÃ§Ãµes** devem ser **idempotentes**;

- Devem ser extraÃ­dos todos os dados fornecidos;

- As extraÃ§Ãµes devem escrever os dados no formato CSV para seu FIleSystem Local seguindo o padrÃ£o de nomenclatura:
    - ano-mÃªs-dia / fonte-de-dados / nome-da-tabela-ou-csv . csv

- As etapas de **extraÃ§Ã£o** de dados devem ocorrer **uma em paralelo Ã  outra;**

- A etapa de carregamento no Data Warehouse deve ocorrer somente se ambas extraÃ§Ãµes tenham sucesso;

- O **pipeline** deve ser **executado** todos os dias **Ã s 04:35 da manhÃ£;**

- O projeto deve ser reproduzÃ­vel em outros ambientes.

---

## ğŸ¯ Arquitetura Utilizada
A soluÃ§Ã£o foi construÃ­da usando o **Docker** para isolar os serviÃ§os e facilitar a execuÃ§Ã£o do desafio:

- **db** --> Banco de origem (PostgreSQL) com dados iniciais carregados a partir de `banvic.sql`.
- **dw** --> Data Warehouse (PostgreSQL) criado a partir de `dw_ddl.sql`.
- **Airflow**: Orquestra execuÃ§Ã£o diÃ¡ria dos scripts:
  - `extracao_postgres/extracao_completa.py`
  - `extracao_csv/extracao_transacoes.py`
  - `load_dw.py`

Fluxo da DAG (`banvic_pipeline`):

![diagrama_arquitetura](./assets/arquitetura_dag.png)


---

## ğŸ“‚ Estrutura de Pastas

```
LH_ED_LUCASTEIXEIRA/
â”‚
â”œâ”€â”€ airflow/                     # Configs e DAGs (Airflow)
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â””â”€â”€ banvic_pipeline.py
â”‚
â”œâ”€â”€ docs/                        # Scripts de criaÃ§Ã£o de tabelas (SQL)
â”‚ â”œâ”€â”€ banvic.sql
â”‚ â””â”€â”€ dw_ddl.sql
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extracao_postgres/
â”‚ â”‚ â””â”€â”€ extracao_completa.py
â”‚ â”œâ”€â”€ extracao_csv/
â”‚ â”‚ â””â”€â”€ extracao_transacoes.py
â”‚ â””â”€â”€ load_dw.py
â”‚
â”œâ”€â”€ docker-compose.yml           # Banco de origem e DW
â”œâ”€â”€ docker-compose-airflow.yml   # Stack do Airflow
â”œâ”€â”€ transacoes.csv               # Arquivo CSV de transaÃ§Ãµes
â”‚
â””â”€â”€ YEAR-MONTH-DAY/              # Pastas geradas apÃ³s execuÃ§Ãµes manuais
â”œâ”€â”€ postgres/
â”‚ â”œâ”€â”€ agencias.csv
â”‚ â”œâ”€â”€ clientes.csv
â”‚ â””â”€â”€ ...
â””â”€â”€ csv/
â””â”€â”€ transacoes.csv

```

---

## âš™ï¸ Recursos NecessÃ¡rios
- **Docker** e **Docker Compose** instalados
- **Python 3**
- **Git** (para clonar o repositÃ³rio)

---

## ğŸš€ Como Executar

### 1ï¸âƒ£ Clone do repositÃ³rio
```bash
git clone https://github.com/lucasteixeira03/LH_ED_LUCASTEIXEIRA.git
```

### 2ï¸âƒ£ Subir bancos (origem e DW)
```bash
docker compose up -d
```
### 3ï¸âƒ£ Subir Airflow
```
docker compose -f docker-compose-airflow.yml up -d
```

### 4ï¸âƒ£ Inicializar Airflow (primeira vez)
```
docker compose -f docker-compose-airflow.yml run --rm airflow-init
```

### 5ï¸âƒ£ Acessar Interface
- URL: http://localhost:8080
   - UsuÃ¡rio: airflow_lucas
   - Senha: wolfria123

### 6ï¸âƒ£ Executar a DAG manualmente (banvic_pipeline)
- Clicar em **TRIGGER DAG**
- Aguardar todas as task ficarem verdes

### 7ï¸âƒ£ Validar Resultado
- Conferir pasta gerada com a data do dia;
- Checar que arquivos **.CSV** foram gerados;
- Validar no DW com:
    - SELECT COUNT(*) FROM clientes;
    - SELECT COUNT(*) FROM transacoes;


---

### ğŸ“ObservaÃ§Ãµes Finais
- Pastas de datas foram mantidas para comprovar histÃ³rico de execuÃ§Ãµes no repositÃ³rio.
- Todos os scripts foram testados e orquestrados pelo Airflow.
- O **.gitignore** foi usado para impedir que volumes do Docker e logs sejam enviados ao repositÃ³rio no github.


